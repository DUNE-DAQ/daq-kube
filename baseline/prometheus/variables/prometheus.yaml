---
prometheus_operator_namespace: kube-prometheus
prometheis: # this is the plural of prometheus
  main:
    namespace: {{ monitoring.namespace }}
    name: main
    serviceName: prometheus
    grafanaDatasourceName: prometheus-{{ target.name }}
    url: http://prometheus.{{ monitoring.namespace }}.svc.cluster.local
    port: 9090
    portName: web
    replicas: 1
    retention: 25d
    alertmanagers:
      - namespace: {{ alertmanagers.main.namespace }}
        name: {{ alertmanagers.main.serviceName }}
        port: {{ alertmanagers.main.portName }}

    # try to schedule on same node as alertmanager
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: {{ kubernetes_label.worker }}
              operator: Exists
      podAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 30
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: alertmanager
                  operator: Exists
              topologyKey: kubernetes.io/hostname

    # these limits should probably be tuned a bit
    # but the wal log is memory hungry
    resources:
      requests:
        cpu: 200m
        memory: 64Mi
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 55Gi # this is guess work
    prometheusSpec:
      storage:
        tsdb:
          retentionSize: 50GB

    # these selectors should probably be set to rational filters
    serviceMonitorNamespaceSelector: {}
    serviceMonitorSelector: {}
    podMonitorNamespaceSelector: {}
    podMonitorSelector: {}
    probeNamespaceSelector: {}
    probeSelector: {}
    ruleNamespaceSelector: {}
    ruleSelector: {}
    scrapeConfigNamespaceSelector: {}
    scrapeConfigSelector: {}
