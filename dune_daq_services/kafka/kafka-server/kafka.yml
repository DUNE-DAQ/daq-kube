{% if strimzi.with_kafka_kraft %}
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaNodePool
metadata:
  name: {{ dunedaq.kafka.name }}
  namespace: {{ dunedaq.kafka.namespace }}
  labels:
    strimzi.io/cluster: {{ dunedaq.kafka.name }}
spec:
  replicas: {{ dunedaq.kafka.replicas }}
  roles:
    - controller
    - broker
  storage:
    type: ephemeral
{% endif %}
---
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: {{ dunedaq.kafka.name }}
  namespace: {{ dunedaq.kafka.namespace }}
{% if strimzi.with_kafka_kraft %}
  annotations:
    strimzi.io/kraft: enabled
    strimzi.io/node-pools: enabled
{% endif %}
spec:
  entityOperator:
    userOperator:
      resources: # this is guess work
        requests:
          cpu: 10m
          memory: 16Mi
        limits:
          cpu: 400m
          memory: 512Mi
{% if not strimzi.with_kafka_kraft %}
    topicOperator: # bidirectional topicOperator not supported on KRaft
      resources:
        requests: # this is guess work
          cpu: 10m
          memory: 16Mi
        limits:
          cpu: 400m
          memory: 512Mi
{% endif %}
    template:    
      pod:       
        affinity:
         nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
             nodeSelectorTerms:
             - matchExpressions:
               - key: {{ kubernetes_label.worker }}
                 operator: Exists
  kafkaExporter:
    topicRegex: ".*"
    groupRegex: ".*"
    template:    
      pod:       
        affinity:
         nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
             nodeSelectorTerms:
             - matchExpressions:
               - key: {{ kubernetes_label.worker }}
                 operator: Exists
{% if kafka.with_cruisecontrol %}
{% if dunedaq.kafka.replicas > 1 %}
  cruiseControl:
    template:    
      pod:       
        affinity:
         nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
             nodeSelectorTerms:
             - matchExpressions:
               - key: {{ kubernetes_label.worker }}
                 operator: Exists
{% endif %}
{% endif %}
  kafka:
    replicas: {{ dunedaq.kafka.replicas }}
    resources:
      requests: # this is guess work
        cpu: 100m
        memory: 1Gi
    template:
      pod:
        affinity:
         nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
             nodeSelectorTerms:
             - matchExpressions:
               - key: {{ kubernetes_label.worker }}
                 operator: Exists
    logging:
      type: inline
      loggers:
        kafka.root.logger.level: WARN
    jvmOptions: # Tuning suggest 8Gi max?
      -Xms: 1024m
      -Xmx: 8192m
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
        configuration:
          useServiceDnsDomain: true
      - name: tls
        port: 9093
        type: internal
        tls: true
        configuration:
          useServiceDnsDomain: true
      - name: external
        port:  9094
        type: nodeport
        tls: false
        configuration:
          bootstrap:
            nodePort: 30092
{% if args.inside_kind %}
          brokers:
{% for broker in range(dunedaq.kafka.replicas) %}
          - broker: {{ broker }}
            nodePort: 3119{{ broker }}
            advertisedHost: {{ kafka_hostname }}
{% endfor %}
{% endif %}
    version: "{{ dunedaq.kafka.version }}"
    config:
      inter.broker.protocol.version: "{{ dunedaq.kafka.protocol_version }}"
      auto.create.topics.enable: true
      log.cleanup.policy: "delete" # Cannot use 'compact' due to empty key in our data format
      log.retention.bytes: 6442450944 # no default upper size limit, set to 6Gi
      max.message.bytes: 1048576 # if you change this you need to review the kafka docs
{% if dunedaq.kafka.replicas < 3 %}
      # replication only makes sense if there are 3 or more brokers
      transaction.state.log.min.isr: 1
      min.insync.replicas: 1
      default.replication.factor: 1
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      num.partitions: 1
{% else %}
      # replicate to n/2 for stability at small replica counts
      transaction.state.log.min.isr: {{ (dunedaq.kafka.replicas / 2)  | round | int }}
      min.insync.replicas: {{ (dunedaq.kafka.replicas / 2) | round | int }}
      default.replication.factor: {{ (dunedaq.kafka.replicas / 2) | round | int }}
      offsets.topic.replication.factor: {{ (dunedaq.kafka.replicas / 2) | round | int }}
      transaction.state.log.replication.factor: {{ (dunedaq.kafka.replicas / 2) | round | int }}
      num.partitions: {{ (dunedaq.kafka.replicas / 2) | round | int }}
{% endif %}
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          name: kafka-metrics
          key: kafka-metrics-config.yml
    storage: # no persistance requested
      type: ephemeral
  zookeeper: # this stanza is still required even in KRaft mode
{% if dunedaq.kafka.replicas < 3 %}
    replicas: 1
{% else %}
    replicas: 3
{% endif %}
    storage:
      type: ephemeral
    resources:
      requests: # this is guess work
        cpu: 100m
        memory: 128Mi
      limits:
        memory: 8Gi
        cpu: "2"
